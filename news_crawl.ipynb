{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd858497",
   "metadata": {
    "id": "cd858497"
   },
   "outputs": [],
   "source": [
    "from requests import get, Session, request\n",
    "from requests.compat import urlparse, urlunparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sqlite3\n",
    "from time import time, localtime, sleep, strftime, mktime\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "session = Session()\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
    "         'Referer': 'https://www.yna.co.kr/'}\n",
    "searchDate = '20221201'\n",
    "nday = 365\n",
    "page_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552a4d07",
   "metadata": {
    "id": "552a4d07"
   },
   "outputs": [],
   "source": [
    "def robotParser(domain):\n",
    "    url = urlunparse(urlparse(domain)[:2] + ('',)*4)\n",
    "    url += '/robots.txt'    \n",
    "    pathEnable = dict()\n",
    "    resp = get(url)\n",
    "    if resp.status_code == 200:\n",
    "        agent = None\n",
    "        for line in resp.text.splitlines():\n",
    "            k, *v = line.split(':')\n",
    "            k = k.strip()\n",
    "            v = ':'.join(v).strip()\n",
    "            if k.lower() == 'user-agent':\n",
    "                agent = v\n",
    "                if v not in pathEnable:\n",
    "                    pathEnable[v] = dict()\n",
    "            else:\n",
    "                if k.lower() == 'allow':\n",
    "                    pathEnable[agent][v] = True\n",
    "                else:\n",
    "                    pathEnable[agent][v] = False\n",
    "    else:\n",
    "        pathEnable['*'] = True\n",
    "    return pathEnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0331e6",
   "metadata": {
    "id": "4c0331e6"
   },
   "outputs": [],
   "source": [
    "def canFetch(pathEnable, path):\n",
    "    agent = '*'\n",
    "    path = urlparse(path).path\n",
    "    \n",
    "    if agent in pathEnable:\n",
    "        if path in pathEnable[agent]:\n",
    "            return pathEnable[agent][path]\n",
    "        else:\n",
    "            if path == '/':\n",
    "                return True\n",
    "            else:\n",
    "                return canFetch(pathEnable,\n",
    "                                '/'.join(path.split('/')[:-1]))\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syP_oa-1sf53",
    "outputId": "272c3def-5228-4526-91e4-f43cd5289848"
   },
   "id": "syP_oa-1sf53",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# url = 'https://www.yna.co.kr/search/index'\n",
    "today = datetime.today()\n",
    "theday = datetime.strptime(searchDate, '%Y%m%d')\n",
    "\n",
    "seens = dict()\n",
    "headers={'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'}\n",
    "\n",
    "robots = dict()\n",
    "\n",
    "categories = {'정치': '01', '경제': '02', '사회': '05', '금융증권산업': '03%2C04', '사건사고': '06', '문화': '07', '생활건강': '08', 'IT과학' : '09', '북한': '10', '국제': '11', '스포츠': '12', '연예': '0712'}\n",
    "\n",
    "params = {\n",
    "    'query': '%ED%96%88%EB%8B%A4',\n",
    "    'period': '1y',\n",
    "    'page_no': page_no,\n",
    "    'ctype': 'A',\n",
    "    'from': theday,\n",
    "    'to': theday,\n",
    "    'page_size': 10,\n",
    "    'channel': 'basic_kr'\n",
    "}"
   ],
   "metadata": {
    "id": "dSRuI4ZeqX5T"
   },
   "id": "dSRuI4ZeqX5T",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# aday = 60*60*24\n",
    "\n",
    "# days_ago = today - timedelta(days=1)\n",
    "# endday = days_ago.strftime('%Y%m%d')\n",
    "# print(endday)\n",
    "\n",
    "for t in range(nday):\n",
    "    days_ago = today - timedelta(days=t)\n",
    "    theday = days_ago.strftime('%Y%m%d')\n",
    "    break\n",
    "\n",
    "\n",
    "for cate, code in categories.items():\n",
    "    params = {\n",
    "    'query': cate,\n",
    "    'ctype': 'A',\n",
    "    'from': theday,\n",
    "    'to': theday,\n",
    "    'period': 'diy',\n",
    "    'div_code': code\n",
    "}"
   ],
   "metadata": {
    "id": "8O5Dpyk1QBw_"
   },
   "id": "8O5Dpyk1QBw_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "resp = get(url, headers=headers) # 깊이 제한\n",
    "# resp.status_code, resp.headers \n",
    "resp_json = json.loads(resp.text)\n",
    "# type(resp_json), resp_json['KR_ARTICLE']['result']\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKLM77XZu3IF",
    "outputId": "05a5f063-c75d-483b-dbb6-09f94fd2ed78"
   },
   "id": "LKLM77XZu3IF",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'decompose'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [42], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m page_resp \u001B[38;5;241m=\u001B[39m get(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.yna.co.kr/view/AKR20221205120300009\u001B[39m\u001B[38;5;124m'\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[0;32m      2\u001B[0m dom \u001B[38;5;241m=\u001B[39m BeautifulSoup(page_resp\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m content \u001B[38;5;241m=\u001B[39m dom\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124marticle.story-news.article p\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mdecompose()\n\u001B[0;32m      4\u001B[0m content[\u001B[38;5;241m2\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m],\u001B[38;5;28mlen\u001B[39m(content)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\bs4\\element.py:2289\u001B[0m, in \u001B[0;36mResultSet.__getattr__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   2287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m   2288\u001B[0m     \u001B[38;5;124;03m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 2289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   2290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResultSet object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. You\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m key\n\u001B[0;32m   2291\u001B[0m     )\n",
      "\u001B[1;31mAttributeError\u001B[0m: ResultSet object has no attribute 'decompose'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "page_resp = get('https://www.yna.co.kr/view/AKR20221205120300009', headers=headers)\n",
    "dom = BeautifulSoup(page_resp.text, 'html.parser')\n",
    "content = dom.select('article.story-news.article p')\n",
    "content[2:-2],len(content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "while True:\n",
    "    url = 'https://ars.yna.co.kr/api/v2/search.asis?query=%ED%96%88%EB%8B%A4&page_no={}&period=1y&from=20211206&to=20221206&ctype=A&page_size=10&channel=basic_kr'.format(page_no)\n",
    "    resp = get(url, params=params, headers=headers) # 깊이 제한\n",
    "    if resp.status_code != 200:\n",
    "        break\n",
    "\n",
    "    ids = json.loads(resp.text)['KR_ARTICLE']['result']\n",
    "    for id in ids:\n",
    "        contents_id = id['CONTENTS_ID']\n",
    "        if contents_id in seens:\n",
    "            break\n",
    "        else:\n",
    "            seens[contents_id] = id['DIST_DATE']\n",
    "# get each pages\n",
    "        page_url = 'https://www.yna.co.kr/view/{}'.format(contents_id)\n",
    "        page_resp = get(page_url, headers=headers)\n",
    "        dom = BeautifulSoup(page_resp.text, 'html.parser')\n",
    "        cate = dom.select_one('meta[property=\"article:section\"]')['content']\n",
    "        title = dom.select_one('h1.tit')\n",
    "        dateNews = dom.select_one('#newsUpdateTime01')['data-pulished-time']\n",
    "        if dateNews != id['DIST_DATE']+id['DIST_TIME']:\n",
    "            print('different date at: ', contents_id)\n",
    "\n",
    "        # get rid off not content texts(info of journalist)\n",
    "        content = dom.select('article.story-news.article p')[2:-2]\n",
    "        content = [at.decompose() for at in content]\n",
    "\n",
    "\n",
    "    page_no += 1\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# crawl to db\n",
    "con = sqlite3.connect('news_Crawl')\n",
    "cur = con.cursor()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "\tINSERT INTO news\n",
    "\t(URL, title, content, dateNews, keywords, category, dataInput)\n",
    "\tVALUES (:url, :title, :content, :dateNews, :keywords, :category, :dataInput);\n",
    "''', {'url': url, 'title': hList, 'content': news, 'dateNews': dateNews, 'keywords': keywords, 'category': cate})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5539a46",
   "metadata": {
    "id": "f5539a46",
    "outputId": "e65d6514-acf2-4095-9635-eaa8db2e8020",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "while urls:\n",
    "    # URLs Pool\n",
    "    seed = urls.pop(-1) # Queue=BFS, Stack=DFS\n",
    "    seens.append(seed[0]) # 깊이 제한\n",
    "    \n",
    "    # Robots.txt\n",
    "#     if seed not in robots:\n",
    "#         robots[seed] = robotParser(seed)\n",
    "#     rp = robots[seed]\n",
    "#     print('[Robots.txt]', seed, canFetch(rp, seed)) # True일때만, 지금은무시\n",
    "    \n",
    "    # Focused Crawling\n",
    "#     if seed[1] > 2: # 자의적 => 휴리스틱\n",
    "#         continue\n",
    "        \n",
    "    resp = get(seed[0], headers=headers) # 깊이 제한\n",
    "    if resp.status_code == 200 and 'Content-Type'.lower() in resp.headers:\n",
    "                                   # content-type이 있을때만\n",
    "        if 'text/html' in\\\n",
    "           [_.strip() for _ in resp.headers['Content-Type'].split(';')]: #바꿔야함\n",
    "            dom = BeautifulSoup(resp.text, 'html.parser')\n",
    "            domlist = dom.select('#article_list.search_news_list.dis_none')\n",
    "            aList = dom.select('#article_list.cts_atclst a')\n",
    "            hList = dom.select('#article_list.cts_atclst .tt2')\n",
    "            news = dom.select_one('.story-news\\ article p')\n",
    "            if len(aList) > 0:\n",
    "                for el in aList[1:6]:\n",
    "                    url = el.attrs['href']\n",
    "                    nextUrl = urljoin(seed[0], url)\n",
    "                    urlParams = urlparse(nextUrl)\n",
    "\n",
    "                    # URL 체크\n",
    "                    if nextUrl not in seens and\\\n",
    "                       nextUrl not in [_[0] for _ in urls] and\\\n",
    "                       nextUrl.startswith('http'):\n",
    "                        urls.append((nextUrl, seed[1]+1))\n",
    "            if len(hList) > 0:\n",
    "                for el in hList:\n",
    "                    url = el.attrs['href']\n",
    "                    nextUrl = urljoin(seed[0], url)\n",
    "                    urlParams = urlparse(nextUrl)\n",
    "# https://n.news.naver.com/mnews/article/119/0002640090?sid=102\n",
    "                    # URL 체크\n",
    "                    if nextUrl not in seens and\\\n",
    "                       nextUrl not in [_[0] for _ in urls] and\\\n",
    "                       nextUrl.startswith('http'):\n",
    "                        urls.append((nextUrl, seed[1]+1))\n",
    "            news = dom.select_one('#ct #contents')\n",
    "            if news:\n",
    "                fileName = re.search('(\\d{10})[?]sid=(\\d{3})$', resp.url)\n",
    "                with open(\n",
    "                    '/content/drive/MyDrive/dataset/paper/{}-{}.txt'.format(fileName.group(2),\n",
    "                                              fileName.group(1)),\n",
    "                    'w', encoding='utf8') as f:\n",
    "                    f.write(news.text.strip())\n",
    "                for img in news.select('img[data-src]'):\n",
    "                    urls.append((urljoin(resp.url, img.attrs['data-src']), seed[1]+1))\n",
    "#             for el in dom.select('[src], [href]'): # [:10] 너비제한\n",
    "#                 url = el.attrs['src' if 'src' in el.attrs else 'href']\n",
    "#                 nextUrl = urljoin(seed[0], url) # 깊이 제한\n",
    "#                 urlParams = urlparse(nextUrl)\n",
    "\n",
    "#                 # URL 체크\n",
    "#                 if nextUrl not in seens and\\\n",
    "#                    nextUrl not in [_[0] for _ in urls] and\\\n",
    "#                    urlParams.netloc == 'blog.naver.com' and\\\n",
    "#                    nextUrl.startswith('http'):\n",
    "#                     # javascript, #fragment 제외시켜야 함\n",
    "#                     # Focused = 특정 도메인으로 제한\n",
    "#                     urls.append((nextUrl, seed[1]+1)) # 깊이 제한\n",
    "        else:\n",
    "            #image/jpeg\n",
    "            ext = re.search(r'image/(png|jpeg|bmp|gif)', resp.headers['Content-Type'])\n",
    "            if ext:\n",
    "#                 fileName = re.search('(\\d{8}_\\d{3})', resp.url)\n",
    "                fileName = resp.url.split('/')[-1]\n",
    "                fileName += '.'+ ext.group(1)\n",
    "                with open('/content/drive/MyDrive/dataset/news/'+fileName, 'wb') as fp:\n",
    "                    fp.write(resp.content)\n",
    "    print(len(urls), len(seens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
