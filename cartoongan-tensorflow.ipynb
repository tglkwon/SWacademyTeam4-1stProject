{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMYh4kqq6KPn2vb5Fn99itm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CzPfYW6wcy2O"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_XZYcY0c5he","executionInfo":{"status":"ok","timestamp":1667453521571,"user_tz":-540,"elapsed":17249,"user":{"displayName":"지능정보 SW아카데미4조","userId":"12526451333221059499"}},"outputId":"098106cf-d87c-47e6-d0e4-0c2ea7660ff2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os"],"metadata":{"id":"6Q4EEoEIdyPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/models/CartoonGan-tensorflow')"],"metadata":{"id":"7p3c1eZ-fOSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHqxCuFOgH5z","executionInfo":{"status":"ok","timestamp":1667312124147,"user_tz":-540,"elapsed":6317,"user":{"displayName":"지능정보 SW아카데미4조","userId":"12526451333221059499"}},"outputId":"5ac83b99-2b71-4ba0-96c3-595435fe2e95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-9emz48pw\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-9emz48pw\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.9.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=f092d86c452a290d05cd19aa65a4be54d7950f72a12229210172f7b6b04860e1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9jv7u5mc/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"]}]},{"cell_type":"code","source":["!pip install \"git+https://github.com/tqdm/tqdm.git@devel#egg=tqdm\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aly6MfCEjAZE","executionInfo":{"status":"ok","timestamp":1667312165422,"user_tz":-540,"elapsed":12468,"user":{"displayName":"지능정보 SW아카데미4조","userId":"12526451333221059499"}},"outputId":"0955fb7f-8bad-4b58-8874-a432ba0b29df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tqdm\n","  Cloning https://github.com/tqdm/tqdm.git (to revision devel) to /tmp/pip-install-p5pfm9l2/tqdm_81996ad78266473486dff7e9f7615250\n","  Running command git clone -q https://github.com/tqdm/tqdm.git /tmp/pip-install-p5pfm9l2/tqdm_81996ad78266473486dff7e9f7615250\n","  Running command git checkout -b devel --track origin/devel\n","  Switched to a new branch 'devel'\n","  Branch 'devel' set up to track remote branch 'devel' from 'origin'.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: tqdm\n","  Building wheel for tqdm (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tqdm: filename=tqdm-4.64.1.dev3+g7541862-py2.py3-none-any.whl size=78685 sha256=bb61df0615cb617021b73dcebd696f2212929e51dfd0d4bbab73864a3e7dac1c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fwewyoia/wheels/79/b3/1f/08fe2f42aa25206609a7a683d155aba489b0369e8f398d2f29\n","Successfully built tqdm\n","Installing collected packages: tqdm\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.1\n","    Uninstalling tqdm-4.64.1:\n","      Successfully uninstalled tqdm-4.64.1\n","Successfully installed tqdm-4.64.1.dev3+g7541862\n"]}]},{"cell_type":"code","source":["!python train.py --batch_size 8 --pretrain_epochs 1 --content_lambda .4 --pretrain_learning_rate 2e-4 --g_adv_lambda 8. --generator_lr 8e-5 --discriminator_lr 3e-5 --style_lambda 25. --light --dataset_name popeye"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0tskX1xfTXj","outputId":"71b81df9-a3d1-43f7-a303-53bd2220a3d1","executionInfo":{"status":"ok","timestamp":1667313875436,"user_tz":-540,"elapsed":1707417,"user":{"displayName":"지능정보 SW아카데미4조","userId":"12526451333221059499"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-11-01 14:16:07.708581: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","[2022-11-01 14:16:11] [Trainer] [INFO] Setting up VGG19 for computing content loss...\n","2022-11-01 14:16:11.576857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-11-01 14:16:13.541297: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2022-11-01 14:16:13.541371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38368 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80134624/80134624 [==============================] - 0s 0us/step\n","[2022-11-01 14:16:14] [Trainer] [INFO] Setting up objective functions and metrics using lsgan...\n","[2022-11-01 14:16:14] [Trainer] [INFO] Setting up checkpoint paths...\n","[2022-11-01 14:16:14] [Trainer] [INFO] Starting to pretrain generator with 1 epochs...\n","[2022-11-01 14:16:14] [Trainer] [INFO] Building `popeye` dataset with domain `A`...\n","[2022-11-01 14:16:16] [Trainer] [INFO] Found 196 domainA images in trainA folder.\n","WARNING:tensorflow:From train.py:221: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:221: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:229: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:229: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","[2022-11-01 14:16:17] [Trainer] [INFO] Initializing generator with batch_size: 8, input_size: 256...\n","Model: \"Generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," FlatConv (FlatConv)         multiple                  7298      \n","                                                                 \n"," DownShuffleUnitV2 (DownShuf  multiple                 29384     \n"," fleUnitV2)                                                      \n","                                                                 \n"," DownShuffleUnitV2 (DownShuf  multiple                 114056    \n"," fleUnitV2)                                                      \n","                                                                 \n"," sequential_15 (Sequential)  (8, 64, 64, 384)          603696    \n","                                                                 \n"," UpSampleConv (UpSampleConv)  multiple                 93222     \n","                                                                 \n"," UpSampleConv (UpSampleConv)  multiple                 23574     \n","                                                                 \n"," sequential_20 (Sequential)  (8, 256, 256, 3)          7203      \n","                                                                 \n"," activation (Activation)     multiple                  0         \n","                                                                 \n","=================================================================\n","Total params: 878,433\n","Trainable params: 878,433\n","Non-trainable params: 0\n","_________________________________________________________________\n","[2022-11-01 14:16:21] [Trainer] [INFO] Setting up optimizer to update generator's parameters...\n","[2022-11-01 14:16:21] [Trainer] [INFO] Try restoring checkpoint: `training_checkpoints/pretrain/pretrain_generator`...\n","[2022-11-01 14:16:21] [Trainer] [INFO] Checkpoint is not found, training from scratch with 1 epochs...\n","WARNING:tensorflow:From train.py:376: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","WARNING:tensorflow:From train.py:376: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","[2022-11-01 14:16:30] [Trainer] [INFO] Starting pre-training loop, setting up summary writer to record progress on TensorBoard...\n","Pretrain Epoch 1/1:   0% 0/25 [00:00<?, ?it/s]2022-11-01 14:16:41.606456: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n","Pretrain Epoch 1/1: 100% 25/25 [00:31<00:00,  1.27s/it]\n","2022-11-01 14:17:02.846797: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","[2022-11-01 14:17:05] [Trainer] [INFO] Saving checkpoints after epoch 1 ended...\n","[2022-11-01 14:17:06] [Trainer] [INFO] Setting up summary writer to record progress on TensorBoard...\n","[2022-11-01 14:17:06] [Trainer] [INFO] Starting adversarial training with 100 epochs, batch size: 8...\n","[2022-11-01 14:17:06] [Trainer] [INFO] Building `popeye` datasets for source/target/smooth domains...\n","[2022-11-01 14:17:06] [Trainer] [INFO] Found 196 domainA images in trainA folder.\n","[2022-11-01 14:17:06] [Trainer] [INFO] Found 6279 domainB images in trainB folder.\n","[2022-11-01 14:17:06] [Trainer] [INFO] Found 6279 domainB_smooth images in trainB_smooth folder.\n","[2022-11-01 14:17:06] [Trainer] [INFO] Setting up optimizer to update generator and discriminator...\n","[2022-11-01 14:17:06] [Trainer] [INFO] Initializing generator with batch_size: 8, input_size: 256...\n","[2022-11-01 14:17:10] [Trainer] [INFO] Searching existing checkpoints: `training_checkpoints/generator/generator`...\n","[2022-11-01 14:17:10] [Trainer] [WARNING] No checkpoint specified (save_path=None); nothing is being restored.\n","[2022-11-01 14:17:10] [Trainer] [WARNING] Previous checkpoints are not found, trying to load checkpoints from pretraining...\n","[2022-11-01 14:17:10] [Trainer] [INFO] Successfully loaded `training_checkpoints/pretrain/pretrain_generator`...\n","[2022-11-01 14:17:10] [Trainer] [INFO] Initializing discriminator with batch_size: 8, input_size: 256...\n","[2022-11-01 14:17:11] [Trainer] [INFO] Searching existing checkpoints: `training_checkpoints/discriminator/discriminator`...\n","[2022-11-01 14:17:11] [Trainer] [INFO] specified checkpoint is not found, training from scratch...\n","[2022-11-01 14:17:13] [Trainer] [INFO] Starting training loop...\n","[2022-11-01 14:17:13] [Trainer] [INFO] Number of trained epochs: 0, epochs to be trained: 100, batch size: 8\n","Train 1/100: 100% 25/25 [00:31<00:00,  1.25s/it]\n","[2022-11-01 14:17:47] [Trainer] [INFO] Saving checkpoints after epoch 1 ended...\n","Train 2/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:18:03] [Trainer] [INFO] Saving checkpoints after epoch 2 ended...\n","Train 3/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:18:20] [Trainer] [INFO] Saving checkpoints after epoch 3 ended...\n","Train 4/100: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:18:36] [Trainer] [INFO] Saving checkpoints after epoch 4 ended...\n","Train 5/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:18:52] [Trainer] [INFO] Saving checkpoints after epoch 5 ended...\n","Train 6/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:19:08] [Trainer] [INFO] Saving checkpoints after epoch 6 ended...\n","Train 7/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:19:24] [Trainer] [INFO] Saving checkpoints after epoch 7 ended...\n","Train 8/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:19:40] [Trainer] [INFO] Saving checkpoints after epoch 8 ended...\n","Train 9/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:19:56] [Trainer] [INFO] Saving checkpoints after epoch 9 ended...\n","Train 10/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:20:13] [Trainer] [INFO] Saving checkpoints after epoch 10 ended...\n","Train 11/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:20:29] [Trainer] [INFO] Saving checkpoints after epoch 11 ended...\n","Train 12/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:20:45] [Trainer] [INFO] Saving checkpoints after epoch 12 ended...\n","Train 13/100: 100% 25/25 [00:16<00:00,  1.56it/s]\n","[2022-11-01 14:21:04] [Trainer] [INFO] Saving checkpoints after epoch 13 ended...\n","Train 14/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:21:20] [Trainer] [INFO] Saving checkpoints after epoch 14 ended...\n","Train 15/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:21:36] [Trainer] [INFO] Saving checkpoints after epoch 15 ended...\n","Train 16/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:21:52] [Trainer] [INFO] Saving checkpoints after epoch 16 ended...\n","Train 17/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:22:08] [Trainer] [INFO] Saving checkpoints after epoch 17 ended...\n","Train 18/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:22:25] [Trainer] [INFO] Saving checkpoints after epoch 18 ended...\n","Train 19/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:22:41] [Trainer] [INFO] Saving checkpoints after epoch 19 ended...\n","Train 20/100: 100% 25/25 [00:14<00:00,  1.78it/s]\n","[2022-11-01 14:22:57] [Trainer] [INFO] Saving checkpoints after epoch 20 ended...\n","Train 21/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:23:14] [Trainer] [INFO] Saving checkpoints after epoch 21 ended...\n","Train 22/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:23:30] [Trainer] [INFO] Saving checkpoints after epoch 22 ended...\n","Train 23/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:23:46] [Trainer] [INFO] Saving checkpoints after epoch 23 ended...\n","Train 24/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:24:02] [Trainer] [INFO] Saving checkpoints after epoch 24 ended...\n","Train 25/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:24:19] [Trainer] [INFO] Saving checkpoints after epoch 25 ended...\n","Train 26/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:24:35] [Trainer] [INFO] Saving checkpoints after epoch 26 ended...\n","Train 27/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:24:51] [Trainer] [INFO] Saving checkpoints after epoch 27 ended...\n","Train 28/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:25:07] [Trainer] [INFO] Saving checkpoints after epoch 28 ended...\n","Train 29/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:25:23] [Trainer] [INFO] Saving checkpoints after epoch 29 ended...\n","Train 30/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:25:39] [Trainer] [INFO] Saving checkpoints after epoch 30 ended...\n","Train 31/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:25:56] [Trainer] [INFO] Saving checkpoints after epoch 31 ended...\n","Train 32/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:26:12] [Trainer] [INFO] Saving checkpoints after epoch 32 ended...\n","Train 33/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:26:28] [Trainer] [INFO] Saving checkpoints after epoch 33 ended...\n","Train 34/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:26:44] [Trainer] [INFO] Saving checkpoints after epoch 34 ended...\n","Train 35/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:27:00] [Trainer] [INFO] Saving checkpoints after epoch 35 ended...\n","Train 36/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:27:16] [Trainer] [INFO] Saving checkpoints after epoch 36 ended...\n","Train 37/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:27:33] [Trainer] [INFO] Saving checkpoints after epoch 37 ended...\n","Train 38/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:27:49] [Trainer] [INFO] Saving checkpoints after epoch 38 ended...\n","Train 39/100: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:28:05] [Trainer] [INFO] Saving checkpoints after epoch 39 ended...\n","Train 40/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:28:21] [Trainer] [INFO] Saving checkpoints after epoch 40 ended...\n","Train 41/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:28:37] [Trainer] [INFO] Saving checkpoints after epoch 41 ended...\n","Train 42/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:28:53] [Trainer] [INFO] Saving checkpoints after epoch 42 ended...\n","Train 43/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:29:09] [Trainer] [INFO] Saving checkpoints after epoch 43 ended...\n","Train 44/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:29:26] [Trainer] [INFO] Saving checkpoints after epoch 44 ended...\n","Train 45/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:29:42] [Trainer] [INFO] Saving checkpoints after epoch 45 ended...\n","Train 46/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:29:58] [Trainer] [INFO] Saving checkpoints after epoch 46 ended...\n","Train 47/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:30:14] [Trainer] [INFO] Saving checkpoints after epoch 47 ended...\n","Train 48/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:30:30] [Trainer] [INFO] Saving checkpoints after epoch 48 ended...\n","Train 49/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:30:46] [Trainer] [INFO] Saving checkpoints after epoch 49 ended...\n","Train 50/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:31:02] [Trainer] [INFO] Saving checkpoints after epoch 50 ended...\n","Train 51/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:31:19] [Trainer] [INFO] Saving checkpoints after epoch 51 ended...\n","Train 52/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:31:35] [Trainer] [INFO] Saving checkpoints after epoch 52 ended...\n","Train 53/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:31:51] [Trainer] [INFO] Saving checkpoints after epoch 53 ended...\n","Train 54/100: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:32:07] [Trainer] [INFO] Saving checkpoints after epoch 54 ended...\n","Train 55/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:32:23] [Trainer] [INFO] Saving checkpoints after epoch 55 ended...\n","Train 56/100: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:32:39] [Trainer] [INFO] Saving checkpoints after epoch 56 ended...\n","Train 57/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:32:55] [Trainer] [INFO] Saving checkpoints after epoch 57 ended...\n","Train 58/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:33:11] [Trainer] [INFO] Saving checkpoints after epoch 58 ended...\n","Train 59/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:33:27] [Trainer] [INFO] Saving checkpoints after epoch 59 ended...\n","Train 60/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:33:43] [Trainer] [INFO] Saving checkpoints after epoch 60 ended...\n","Train 61/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:34:00] [Trainer] [INFO] Saving checkpoints after epoch 61 ended...\n","Train 62/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:34:16] [Trainer] [INFO] Saving checkpoints after epoch 62 ended...\n","Train 63/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:34:32] [Trainer] [INFO] Saving checkpoints after epoch 63 ended...\n","Train 64/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:34:48] [Trainer] [INFO] Saving checkpoints after epoch 64 ended...\n","Train 65/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:35:04] [Trainer] [INFO] Saving checkpoints after epoch 65 ended...\n","Train 66/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:35:20] [Trainer] [INFO] Saving checkpoints after epoch 66 ended...\n","Train 67/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:35:36] [Trainer] [INFO] Saving checkpoints after epoch 67 ended...\n","Train 68/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:35:53] [Trainer] [INFO] Saving checkpoints after epoch 68 ended...\n","Train 69/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:36:09] [Trainer] [INFO] Saving checkpoints after epoch 69 ended...\n","Train 70/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:36:25] [Trainer] [INFO] Saving checkpoints after epoch 70 ended...\n","Train 71/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:36:41] [Trainer] [INFO] Saving checkpoints after epoch 71 ended...\n","Train 72/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:36:57] [Trainer] [INFO] Saving checkpoints after epoch 72 ended...\n","Train 73/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:37:13] [Trainer] [INFO] Saving checkpoints after epoch 73 ended...\n","Train 74/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:37:30] [Trainer] [INFO] Saving checkpoints after epoch 74 ended...\n","Train 75/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:37:46] [Trainer] [INFO] Saving checkpoints after epoch 75 ended...\n","Train 76/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:38:02] [Trainer] [INFO] Saving checkpoints after epoch 76 ended...\n","Train 77/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:38:18] [Trainer] [INFO] Saving checkpoints after epoch 77 ended...\n","Train 78/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:38:34] [Trainer] [INFO] Saving checkpoints after epoch 78 ended...\n","Train 79/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:38:50] [Trainer] [INFO] Saving checkpoints after epoch 79 ended...\n","Train 80/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:39:07] [Trainer] [INFO] Saving checkpoints after epoch 80 ended...\n","Train 81/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:39:23] [Trainer] [INFO] Saving checkpoints after epoch 81 ended...\n","Train 82/100: 100% 25/25 [00:14<00:00,  1.78it/s]\n","[2022-11-01 14:39:39] [Trainer] [INFO] Saving checkpoints after epoch 82 ended...\n","Train 83/100: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:39:55] [Trainer] [INFO] Saving checkpoints after epoch 83 ended...\n","Train 84/100: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 14:40:11] [Trainer] [INFO] Saving checkpoints after epoch 84 ended...\n","Train 85/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:40:28] [Trainer] [INFO] Saving checkpoints after epoch 85 ended...\n","Train 86/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:40:44] [Trainer] [INFO] Saving checkpoints after epoch 86 ended...\n","Train 87/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:41:00] [Trainer] [INFO] Saving checkpoints after epoch 87 ended...\n","Train 88/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:41:16] [Trainer] [INFO] Saving checkpoints after epoch 88 ended...\n","Train 89/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:41:32] [Trainer] [INFO] Saving checkpoints after epoch 89 ended...\n","Train 90/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:41:48] [Trainer] [INFO] Saving checkpoints after epoch 90 ended...\n","Train 91/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:42:05] [Trainer] [INFO] Saving checkpoints after epoch 91 ended...\n","Train 92/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:42:21] [Trainer] [INFO] Saving checkpoints after epoch 92 ended...\n","Train 93/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:42:37] [Trainer] [INFO] Saving checkpoints after epoch 93 ended...\n","Train 94/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:42:53] [Trainer] [INFO] Saving checkpoints after epoch 94 ended...\n","Train 95/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:43:09] [Trainer] [INFO] Saving checkpoints after epoch 95 ended...\n","Train 96/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:43:25] [Trainer] [INFO] Saving checkpoints after epoch 96 ended...\n","Train 97/100: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 14:43:41] [Trainer] [INFO] Saving checkpoints after epoch 97 ended...\n","Train 98/100: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:43:57] [Trainer] [INFO] Saving checkpoints after epoch 98 ended...\n","Train 99/100: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 14:44:14] [Trainer] [INFO] Saving checkpoints after epoch 99 ended...\n","Train 100/100: 100% 25/25 [00:13<00:00,  1.79it/s]\n","[2022-11-01 14:44:30] [Trainer] [INFO] Saving checkpoints after epoch 100 ended...\n"]}]},{"cell_type":"code","source":["!python train.py --batch_size 8 --pretrain_epochs 1 --content_lambda .4 --pretrain_learning_rate 2e-4 --g_adv_lambda 8. --generator_lr 8e-5 --discriminator_lr 3e-5 --style_lambda 25. --epochs 300 --light --dataset_name popeye"],"metadata":{"id":"gm3YS5P4hZ-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667317288331,"user_tz":-540,"elapsed":3194214,"user":{"displayName":"지능정보 SW아카데미4조","userId":"12526451333221059499"}},"outputId":"9e329066-b52c-4105-e04f-5e65bdd78a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-11-01 14:48:13.344709: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","[2022-11-01 14:48:15] [Trainer] [INFO] Setting up VGG19 for computing content loss...\n","2022-11-01 14:48:15.915361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-11-01 14:48:16.678427: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2022-11-01 14:48:16.678495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38368 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n","[2022-11-01 14:48:17] [Trainer] [INFO] Setting up objective functions and metrics using lsgan...\n","[2022-11-01 14:48:17] [Trainer] [INFO] Setting up checkpoint paths...\n","[2022-11-01 14:48:17] [Trainer] [INFO] Starting to pretrain generator with 1 epochs...\n","[2022-11-01 14:48:17] [Trainer] [INFO] Building `popeye` dataset with domain `A`...\n","[2022-11-01 14:48:17] [Trainer] [INFO] Found 196 domainA images in trainA folder.\n","WARNING:tensorflow:From train.py:221: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:221: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:229: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","WARNING:tensorflow:From train.py:229: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","[2022-11-01 14:48:17] [Trainer] [INFO] Initializing generator with batch_size: 8, input_size: 256...\n","Model: \"Generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," FlatConv (FlatConv)         multiple                  7298      \n","                                                                 \n"," DownShuffleUnitV2 (DownShuf  multiple                 29384     \n"," fleUnitV2)                                                      \n","                                                                 \n"," DownShuffleUnitV2 (DownShuf  multiple                 114056    \n"," fleUnitV2)                                                      \n","                                                                 \n"," sequential_15 (Sequential)  (8, 64, 64, 384)          603696    \n","                                                                 \n"," UpSampleConv (UpSampleConv)  multiple                 93222     \n","                                                                 \n"," UpSampleConv (UpSampleConv)  multiple                 23574     \n","                                                                 \n"," sequential_20 (Sequential)  (8, 256, 256, 3)          7203      \n","                                                                 \n"," activation (Activation)     multiple                  0         \n","                                                                 \n","=================================================================\n","Total params: 878,433\n","Trainable params: 878,433\n","Non-trainable params: 0\n","_________________________________________________________________\n","[2022-11-01 14:48:21] [Trainer] [INFO] Setting up optimizer to update generator's parameters...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Try restoring checkpoint: `training_checkpoints/pretrain/pretrain_generator`...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Previous checkpoints has been restored.\n","[2022-11-01 14:48:21] [Trainer] [INFO] Already trained 1 epochs. Set a larger `pretrain_epochs`...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Setting up summary writer to record progress on TensorBoard...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Starting adversarial training with 300 epochs, batch size: 8...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Building `popeye` datasets for source/target/smooth domains...\n","[2022-11-01 14:48:21] [Trainer] [INFO] Found 196 domainA images in trainA folder.\n","[2022-11-01 14:48:21] [Trainer] [INFO] Found 6279 domainB images in trainB folder.\n","[2022-11-01 14:48:22] [Trainer] [INFO] Found 6279 domainB_smooth images in trainB_smooth folder.\n","[2022-11-01 14:48:22] [Trainer] [INFO] Setting up optimizer to update generator and discriminator...\n","[2022-11-01 14:48:22] [Trainer] [INFO] Initializing generator with batch_size: 8, input_size: 256...\n","[2022-11-01 14:48:25] [Trainer] [INFO] Searching existing checkpoints: `training_checkpoints/generator/generator`...\n","[2022-11-01 14:48:25] [Trainer] [INFO] Previous checkpoints has been restored.\n","[2022-11-01 14:48:25] [Trainer] [INFO] Already trained 101 epochs, 199 epochs left to be trained...\n","[2022-11-01 14:48:25] [Trainer] [INFO] Initializing discriminator with batch_size: 8, input_size: 256...\n","[2022-11-01 14:48:26] [Trainer] [INFO] Searching existing checkpoints: `training_checkpoints/discriminator/discriminator`...\n","[2022-11-01 14:48:26] [Trainer] [INFO] Previous checkpoints has been restored.\n","WARNING:tensorflow:From train.py:550: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","WARNING:tensorflow:From train.py:550: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.map_fn(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n","[2022-11-01 14:48:28] [Trainer] [INFO] Starting training loop...\n","[2022-11-01 14:48:28] [Trainer] [INFO] Number of trained epochs: 101, epochs to be trained: 199, batch size: 8\n","Train 1/199:   0% 0/25 [00:00<?, ?it/s]2022-11-01 14:48:45.805558: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n","Train 1/199: 100% 25/25 [00:34<00:00,  1.37s/it]\n","2022-11-01 14:49:03.318982: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","[2022-11-01 14:49:06] [Trainer] [INFO] Saving checkpoints after epoch 102 ended...\n","Train 2/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:49:22] [Trainer] [INFO] Saving checkpoints after epoch 103 ended...\n","Train 3/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:49:37] [Trainer] [INFO] Saving checkpoints after epoch 104 ended...\n","Train 4/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:49:53] [Trainer] [INFO] Saving checkpoints after epoch 105 ended...\n","Train 5/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:50:09] [Trainer] [INFO] Saving checkpoints after epoch 106 ended...\n","Train 6/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:50:25] [Trainer] [INFO] Saving checkpoints after epoch 107 ended...\n","Train 7/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:50:41] [Trainer] [INFO] Saving checkpoints after epoch 108 ended...\n","Train 8/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:50:56] [Trainer] [INFO] Saving checkpoints after epoch 109 ended...\n","Train 9/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:51:12] [Trainer] [INFO] Saving checkpoints after epoch 110 ended...\n","Train 10/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:51:28] [Trainer] [INFO] Saving checkpoints after epoch 111 ended...\n","Train 11/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:51:44] [Trainer] [INFO] Saving checkpoints after epoch 112 ended...\n","Train 12/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:52:00] [Trainer] [INFO] Saving checkpoints after epoch 113 ended...\n","Train 13/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:52:16] [Trainer] [INFO] Saving checkpoints after epoch 114 ended...\n","Train 14/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:52:32] [Trainer] [INFO] Saving checkpoints after epoch 115 ended...\n","Train 15/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:52:47] [Trainer] [INFO] Saving checkpoints after epoch 116 ended...\n","Train 16/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:53:03] [Trainer] [INFO] Saving checkpoints after epoch 117 ended...\n","Train 17/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:53:19] [Trainer] [INFO] Saving checkpoints after epoch 118 ended...\n","Train 18/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:53:35] [Trainer] [INFO] Saving checkpoints after epoch 119 ended...\n","Train 19/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:53:51] [Trainer] [INFO] Saving checkpoints after epoch 120 ended...\n","Train 20/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:54:06] [Trainer] [INFO] Saving checkpoints after epoch 121 ended...\n","Train 21/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:54:22] [Trainer] [INFO] Saving checkpoints after epoch 122 ended...\n","Train 22/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:54:38] [Trainer] [INFO] Saving checkpoints after epoch 123 ended...\n","Train 23/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:54:54] [Trainer] [INFO] Saving checkpoints after epoch 124 ended...\n","Train 24/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:55:10] [Trainer] [INFO] Saving checkpoints after epoch 125 ended...\n","Train 25/199: 100% 25/25 [00:13<00:00,  1.88it/s]\n","[2022-11-01 14:55:25] [Trainer] [INFO] Saving checkpoints after epoch 126 ended...\n","Train 26/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:55:41] [Trainer] [INFO] Saving checkpoints after epoch 127 ended...\n","Train 27/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:55:57] [Trainer] [INFO] Saving checkpoints after epoch 128 ended...\n","Train 28/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:56:13] [Trainer] [INFO] Saving checkpoints after epoch 129 ended...\n","Train 29/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:56:29] [Trainer] [INFO] Saving checkpoints after epoch 130 ended...\n","Train 30/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:56:45] [Trainer] [INFO] Saving checkpoints after epoch 131 ended...\n","Train 31/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:57:01] [Trainer] [INFO] Saving checkpoints after epoch 132 ended...\n","Train 32/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:57:16] [Trainer] [INFO] Saving checkpoints after epoch 133 ended...\n","Train 33/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:57:32] [Trainer] [INFO] Saving checkpoints after epoch 134 ended...\n","Train 34/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:57:48] [Trainer] [INFO] Saving checkpoints after epoch 135 ended...\n","Train 35/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:58:04] [Trainer] [INFO] Saving checkpoints after epoch 136 ended...\n","Train 36/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 14:58:20] [Trainer] [INFO] Saving checkpoints after epoch 137 ended...\n","Train 37/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 14:58:36] [Trainer] [INFO] Saving checkpoints after epoch 138 ended...\n","Train 38/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:58:51] [Trainer] [INFO] Saving checkpoints after epoch 139 ended...\n","Train 39/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:59:07] [Trainer] [INFO] Saving checkpoints after epoch 140 ended...\n","Train 40/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 14:59:23] [Trainer] [INFO] Saving checkpoints after epoch 141 ended...\n","Train 41/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 14:59:39] [Trainer] [INFO] Saving checkpoints after epoch 142 ended...\n","Train 42/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 14:59:55] [Trainer] [INFO] Saving checkpoints after epoch 143 ended...\n","Train 43/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:00:11] [Trainer] [INFO] Saving checkpoints after epoch 144 ended...\n","Train 44/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:00:27] [Trainer] [INFO] Saving checkpoints after epoch 145 ended...\n","Train 45/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:00:43] [Trainer] [INFO] Saving checkpoints after epoch 146 ended...\n","Train 46/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:00:58] [Trainer] [INFO] Saving checkpoints after epoch 147 ended...\n","Train 47/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:01:14] [Trainer] [INFO] Saving checkpoints after epoch 148 ended...\n","Train 48/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:01:30] [Trainer] [INFO] Saving checkpoints after epoch 149 ended...\n","Train 49/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:01:46] [Trainer] [INFO] Saving checkpoints after epoch 150 ended...\n","Train 50/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:02:02] [Trainer] [INFO] Saving checkpoints after epoch 151 ended...\n","Train 51/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:02:17] [Trainer] [INFO] Saving checkpoints after epoch 152 ended...\n","Train 52/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:02:33] [Trainer] [INFO] Saving checkpoints after epoch 153 ended...\n","Train 53/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:02:49] [Trainer] [INFO] Saving checkpoints after epoch 154 ended...\n","Train 54/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:03:05] [Trainer] [INFO] Saving checkpoints after epoch 155 ended...\n","Train 55/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:03:21] [Trainer] [INFO] Saving checkpoints after epoch 156 ended...\n","Train 56/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:03:37] [Trainer] [INFO] Saving checkpoints after epoch 157 ended...\n","Train 57/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:03:52] [Trainer] [INFO] Saving checkpoints after epoch 158 ended...\n","Train 58/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:04:08] [Trainer] [INFO] Saving checkpoints after epoch 159 ended...\n","Train 59/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:04:24] [Trainer] [INFO] Saving checkpoints after epoch 160 ended...\n","Train 60/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:04:40] [Trainer] [INFO] Saving checkpoints after epoch 161 ended...\n","Train 61/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:04:56] [Trainer] [INFO] Saving checkpoints after epoch 162 ended...\n","Train 62/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:05:12] [Trainer] [INFO] Saving checkpoints after epoch 163 ended...\n","Train 63/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:05:27] [Trainer] [INFO] Saving checkpoints after epoch 164 ended...\n","Train 64/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:05:43] [Trainer] [INFO] Saving checkpoints after epoch 165 ended...\n","Train 65/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:05:59] [Trainer] [INFO] Saving checkpoints after epoch 166 ended...\n","Train 66/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:06:15] [Trainer] [INFO] Saving checkpoints after epoch 167 ended...\n","Train 67/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:06:31] [Trainer] [INFO] Saving checkpoints after epoch 168 ended...\n","Train 68/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:06:46] [Trainer] [INFO] Saving checkpoints after epoch 169 ended...\n","Train 69/199: 100% 25/25 [00:13<00:00,  1.81it/s]\n","[2022-11-01 15:07:02] [Trainer] [INFO] Saving checkpoints after epoch 170 ended...\n","Train 70/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:07:18] [Trainer] [INFO] Saving checkpoints after epoch 171 ended...\n","Train 71/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:07:34] [Trainer] [INFO] Saving checkpoints after epoch 172 ended...\n","Train 72/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:07:50] [Trainer] [INFO] Saving checkpoints after epoch 173 ended...\n","Train 73/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:08:06] [Trainer] [INFO] Saving checkpoints after epoch 174 ended...\n","Train 74/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:08:22] [Trainer] [INFO] Saving checkpoints after epoch 175 ended...\n","Train 75/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:08:37] [Trainer] [INFO] Saving checkpoints after epoch 176 ended...\n","Train 76/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:08:53] [Trainer] [INFO] Saving checkpoints after epoch 177 ended...\n","Train 77/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:09:09] [Trainer] [INFO] Saving checkpoints after epoch 178 ended...\n","Train 78/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:09:25] [Trainer] [INFO] Saving checkpoints after epoch 179 ended...\n","Train 79/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:09:41] [Trainer] [INFO] Saving checkpoints after epoch 180 ended...\n","Train 80/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:09:57] [Trainer] [INFO] Saving checkpoints after epoch 181 ended...\n","Train 81/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:10:12] [Trainer] [INFO] Saving checkpoints after epoch 182 ended...\n","Train 82/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:10:28] [Trainer] [INFO] Saving checkpoints after epoch 183 ended...\n","Train 83/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:10:44] [Trainer] [INFO] Saving checkpoints after epoch 184 ended...\n","Train 84/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:11:00] [Trainer] [INFO] Saving checkpoints after epoch 185 ended...\n","Train 85/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:11:16] [Trainer] [INFO] Saving checkpoints after epoch 186 ended...\n","Train 86/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:11:32] [Trainer] [INFO] Saving checkpoints after epoch 187 ended...\n","Train 87/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:11:48] [Trainer] [INFO] Saving checkpoints after epoch 188 ended...\n","Train 88/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:12:04] [Trainer] [INFO] Saving checkpoints after epoch 189 ended...\n","Train 89/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:12:20] [Trainer] [INFO] Saving checkpoints after epoch 190 ended...\n","Train 90/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:12:35] [Trainer] [INFO] Saving checkpoints after epoch 191 ended...\n","Train 91/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:12:51] [Trainer] [INFO] Saving checkpoints after epoch 192 ended...\n","Train 92/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:13:07] [Trainer] [INFO] Saving checkpoints after epoch 193 ended...\n","Train 93/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:13:23] [Trainer] [INFO] Saving checkpoints after epoch 194 ended...\n","Train 94/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:13:39] [Trainer] [INFO] Saving checkpoints after epoch 195 ended...\n","Train 95/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:13:54] [Trainer] [INFO] Saving checkpoints after epoch 196 ended...\n","Train 96/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:14:10] [Trainer] [INFO] Saving checkpoints after epoch 197 ended...\n","Train 97/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:14:26] [Trainer] [INFO] Saving checkpoints after epoch 198 ended...\n","Train 98/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:14:42] [Trainer] [INFO] Saving checkpoints after epoch 199 ended...\n","Train 99/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:14:58] [Trainer] [INFO] Saving checkpoints after epoch 200 ended...\n","Train 100/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:15:14] [Trainer] [INFO] Saving checkpoints after epoch 201 ended...\n","Train 101/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:15:29] [Trainer] [INFO] Saving checkpoints after epoch 202 ended...\n","Train 102/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:15:45] [Trainer] [INFO] Saving checkpoints after epoch 203 ended...\n","Train 103/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:16:01] [Trainer] [INFO] Saving checkpoints after epoch 204 ended...\n","Train 104/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:16:17] [Trainer] [INFO] Saving checkpoints after epoch 205 ended...\n","Train 105/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:16:33] [Trainer] [INFO] Saving checkpoints after epoch 206 ended...\n","Train 106/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:16:49] [Trainer] [INFO] Saving checkpoints after epoch 207 ended...\n","Train 107/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:17:05] [Trainer] [INFO] Saving checkpoints after epoch 208 ended...\n","Train 108/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:17:20] [Trainer] [INFO] Saving checkpoints after epoch 209 ended...\n","Train 109/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:17:36] [Trainer] [INFO] Saving checkpoints after epoch 210 ended...\n","Train 110/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:17:52] [Trainer] [INFO] Saving checkpoints after epoch 211 ended...\n","Train 111/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:18:08] [Trainer] [INFO] Saving checkpoints after epoch 212 ended...\n","Train 112/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:18:24] [Trainer] [INFO] Saving checkpoints after epoch 213 ended...\n","Train 113/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:18:39] [Trainer] [INFO] Saving checkpoints after epoch 214 ended...\n","Train 114/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:18:55] [Trainer] [INFO] Saving checkpoints after epoch 215 ended...\n","Train 115/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:19:11] [Trainer] [INFO] Saving checkpoints after epoch 216 ended...\n","Train 116/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:19:27] [Trainer] [INFO] Saving checkpoints after epoch 217 ended...\n","Train 117/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:19:43] [Trainer] [INFO] Saving checkpoints after epoch 218 ended...\n","Train 118/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:19:59] [Trainer] [INFO] Saving checkpoints after epoch 219 ended...\n","Train 119/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:20:14] [Trainer] [INFO] Saving checkpoints after epoch 220 ended...\n","Train 120/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:20:30] [Trainer] [INFO] Saving checkpoints after epoch 221 ended...\n","Train 121/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:20:46] [Trainer] [INFO] Saving checkpoints after epoch 222 ended...\n","Train 122/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:21:02] [Trainer] [INFO] Saving checkpoints after epoch 223 ended...\n","Train 123/199: 100% 25/25 [00:13<00:00,  1.80it/s]\n","[2022-11-01 15:21:18] [Trainer] [INFO] Saving checkpoints after epoch 224 ended...\n","Train 124/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:21:34] [Trainer] [INFO] Saving checkpoints after epoch 225 ended...\n","Train 125/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:21:50] [Trainer] [INFO] Saving checkpoints after epoch 226 ended...\n","Train 126/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:22:05] [Trainer] [INFO] Saving checkpoints after epoch 227 ended...\n","Train 127/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:22:21] [Trainer] [INFO] Saving checkpoints after epoch 228 ended...\n","Train 128/199: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 15:22:37] [Trainer] [INFO] Saving checkpoints after epoch 229 ended...\n","Train 129/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:22:53] [Trainer] [INFO] Saving checkpoints after epoch 230 ended...\n","Train 130/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:23:09] [Trainer] [INFO] Saving checkpoints after epoch 231 ended...\n","Train 131/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:23:25] [Trainer] [INFO] Saving checkpoints after epoch 232 ended...\n","Train 132/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:23:41] [Trainer] [INFO] Saving checkpoints after epoch 233 ended...\n","Train 133/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:23:57] [Trainer] [INFO] Saving checkpoints after epoch 234 ended...\n","Train 134/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:24:13] [Trainer] [INFO] Saving checkpoints after epoch 235 ended...\n","Train 135/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:24:28] [Trainer] [INFO] Saving checkpoints after epoch 236 ended...\n","Train 136/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:24:44] [Trainer] [INFO] Saving checkpoints after epoch 237 ended...\n","Train 137/199: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 15:25:00] [Trainer] [INFO] Saving checkpoints after epoch 238 ended...\n","Train 138/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:25:16] [Trainer] [INFO] Saving checkpoints after epoch 239 ended...\n","Train 139/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:25:32] [Trainer] [INFO] Saving checkpoints after epoch 240 ended...\n","Train 140/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:25:48] [Trainer] [INFO] Saving checkpoints after epoch 241 ended...\n","Train 141/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:26:04] [Trainer] [INFO] Saving checkpoints after epoch 242 ended...\n","Train 142/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:26:20] [Trainer] [INFO] Saving checkpoints after epoch 243 ended...\n","Train 143/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:26:35] [Trainer] [INFO] Saving checkpoints after epoch 244 ended...\n","Train 144/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:26:51] [Trainer] [INFO] Saving checkpoints after epoch 245 ended...\n","Train 145/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:27:07] [Trainer] [INFO] Saving checkpoints after epoch 246 ended...\n","Train 146/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:27:23] [Trainer] [INFO] Saving checkpoints after epoch 247 ended...\n","Train 147/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:27:39] [Trainer] [INFO] Saving checkpoints after epoch 248 ended...\n","Train 148/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:27:55] [Trainer] [INFO] Saving checkpoints after epoch 249 ended...\n","Train 149/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:28:11] [Trainer] [INFO] Saving checkpoints after epoch 250 ended...\n","Train 150/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:28:27] [Trainer] [INFO] Saving checkpoints after epoch 251 ended...\n","Train 151/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:28:43] [Trainer] [INFO] Saving checkpoints after epoch 252 ended...\n","Train 152/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:28:58] [Trainer] [INFO] Saving checkpoints after epoch 253 ended...\n","Train 153/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:29:14] [Trainer] [INFO] Saving checkpoints after epoch 254 ended...\n","Train 154/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:29:30] [Trainer] [INFO] Saving checkpoints after epoch 255 ended...\n","Train 155/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:29:46] [Trainer] [INFO] Saving checkpoints after epoch 256 ended...\n","Train 156/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:30:02] [Trainer] [INFO] Saving checkpoints after epoch 257 ended...\n","Train 157/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:30:18] [Trainer] [INFO] Saving checkpoints after epoch 258 ended...\n","Train 158/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:30:34] [Trainer] [INFO] Saving checkpoints after epoch 259 ended...\n","Train 159/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:30:50] [Trainer] [INFO] Saving checkpoints after epoch 260 ended...\n","Train 160/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:31:05] [Trainer] [INFO] Saving checkpoints after epoch 261 ended...\n","Train 161/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:31:21] [Trainer] [INFO] Saving checkpoints after epoch 262 ended...\n","Train 162/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:31:37] [Trainer] [INFO] Saving checkpoints after epoch 263 ended...\n","Train 163/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:31:53] [Trainer] [INFO] Saving checkpoints after epoch 264 ended...\n","Train 164/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:32:09] [Trainer] [INFO] Saving checkpoints after epoch 265 ended...\n","Train 165/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:32:24] [Trainer] [INFO] Saving checkpoints after epoch 266 ended...\n","Train 166/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:32:40] [Trainer] [INFO] Saving checkpoints after epoch 267 ended...\n","Train 167/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:32:56] [Trainer] [INFO] Saving checkpoints after epoch 268 ended...\n","Train 168/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:33:12] [Trainer] [INFO] Saving checkpoints after epoch 269 ended...\n","Train 169/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:33:28] [Trainer] [INFO] Saving checkpoints after epoch 270 ended...\n","Train 170/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:33:44] [Trainer] [INFO] Saving checkpoints after epoch 271 ended...\n","Train 171/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:33:59] [Trainer] [INFO] Saving checkpoints after epoch 272 ended...\n","Train 172/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:34:15] [Trainer] [INFO] Saving checkpoints after epoch 273 ended...\n","Train 173/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:34:31] [Trainer] [INFO] Saving checkpoints after epoch 274 ended...\n","Train 174/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:34:47] [Trainer] [INFO] Saving checkpoints after epoch 275 ended...\n","Train 175/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:35:03] [Trainer] [INFO] Saving checkpoints after epoch 276 ended...\n","Train 176/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:35:19] [Trainer] [INFO] Saving checkpoints after epoch 277 ended...\n","Train 177/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:35:34] [Trainer] [INFO] Saving checkpoints after epoch 278 ended...\n","Train 178/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:35:50] [Trainer] [INFO] Saving checkpoints after epoch 279 ended...\n","Train 179/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:36:06] [Trainer] [INFO] Saving checkpoints after epoch 280 ended...\n","Train 180/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:36:22] [Trainer] [INFO] Saving checkpoints after epoch 281 ended...\n","Train 181/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:36:38] [Trainer] [INFO] Saving checkpoints after epoch 282 ended...\n","Train 182/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:36:54] [Trainer] [INFO] Saving checkpoints after epoch 283 ended...\n","Train 183/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:37:10] [Trainer] [INFO] Saving checkpoints after epoch 284 ended...\n","Train 184/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:37:26] [Trainer] [INFO] Saving checkpoints after epoch 285 ended...\n","Train 185/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:37:41] [Trainer] [INFO] Saving checkpoints after epoch 286 ended...\n","Train 186/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:37:57] [Trainer] [INFO] Saving checkpoints after epoch 287 ended...\n","Train 187/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:38:13] [Trainer] [INFO] Saving checkpoints after epoch 288 ended...\n","Train 188/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:38:29] [Trainer] [INFO] Saving checkpoints after epoch 289 ended...\n","Train 189/199: 100% 25/25 [00:13<00:00,  1.84it/s]\n","[2022-11-01 15:38:45] [Trainer] [INFO] Saving checkpoints after epoch 290 ended...\n","Train 190/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:39:01] [Trainer] [INFO] Saving checkpoints after epoch 291 ended...\n","Train 191/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:39:17] [Trainer] [INFO] Saving checkpoints after epoch 292 ended...\n","Train 192/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:39:32] [Trainer] [INFO] Saving checkpoints after epoch 293 ended...\n","Train 193/199: 100% 25/25 [00:13<00:00,  1.83it/s]\n","[2022-11-01 15:39:49] [Trainer] [INFO] Saving checkpoints after epoch 294 ended...\n","Train 194/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:40:04] [Trainer] [INFO] Saving checkpoints after epoch 295 ended...\n","Train 195/199: 100% 25/25 [00:13<00:00,  1.86it/s]\n","[2022-11-01 15:40:20] [Trainer] [INFO] Saving checkpoints after epoch 296 ended...\n","Train 196/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:40:36] [Trainer] [INFO] Saving checkpoints after epoch 297 ended...\n","Train 197/199: 100% 25/25 [00:13<00:00,  1.82it/s]\n","[2022-11-01 15:40:52] [Trainer] [INFO] Saving checkpoints after epoch 298 ended...\n","Train 198/199: 100% 25/25 [00:13<00:00,  1.85it/s]\n","[2022-11-01 15:41:08] [Trainer] [INFO] Saving checkpoints after epoch 299 ended...\n","Train 199/199: 100% 25/25 [00:13<00:00,  1.87it/s]\n","[2022-11-01 15:41:23] [Trainer] [INFO] Saving checkpoints after epoch 300 ended...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xn2X_F5YqguX"},"execution_count":null,"outputs":[]}]}